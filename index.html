<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations"/>
  <meta property="og:description" content="Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations"/>
  <meta property="og:url" content="https://steerers.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/main.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations">
  <meta name="twitter:description" content="Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/main.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations</title>
  <link rel="icon" type="image/x-icon" href="static/image/bu_logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations</h1>
            <div class="is-size-4 publication-authors">
              <!-- Paper authors -->
              <span class="author-block", style="padding-left:150px">
                <a href="https://kim-dahye.github.io/" target="_blank">Dahye Kim<sup>1</sup></a></span>
                <span class="author-block", style="padding-left:180px">
                  <a href="https://deeptigp.github.io/" target="_blank">Deepti Ghadiyaram<sup>1 2</sup></a></span>
                  </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Boston University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>Runway<br>arXiv 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/kim-dahye/steerers" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="body">
    <div class="container">
      <center>
        <img src="./static/images/main.png" width="90%">
      </center>      

      <div class="content has-text-justified" style="margin: auto; width: 90%;">
          <b>Monosemantic interpretable concepts</b> such as nudity, photographic styles, and object attributes are identified using k-sparse autoencoders (k-SAE). We leverage them to enable precise modification of a desired concept during the generation process, without impacting the overall image structure, photo-realism, visual quality, and prompt alignment (for safe concepts). Our framework can be used to remove unsafe concepts (top row), photographic styles (middle row), and object attributes (last row).
      </div>
      <br>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and/or compromise generation quality. In this work, we propose a novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models. Specifically, we first identify interpretable monosemantic concepts in the latent space of text embeddings and leverage them to precisely steer the generation away or towards a given concept (<em>e.g.,</em> nudity) or to introduce a new concept (<em>e.g.,</em> photographic style). Through extensive experiments, we demonstrate that our approach is very simple, requires no retraining of the base model nor LoRA adapters, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Our method yields an improvement of <b>20.01%</b> in unsafe concept removal, is effective in style manipulation, and is <b>~5x</b> faster than current state-of-the-art.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Approach</h2>
        <div class="content has-text-justified">
          <img src="static/images/figure2.png" width="60%" style="display: block; margin: auto;">
        </div>
        <p class="content has-text-justified">
          <b>K-sparse autoencoder (k-SAE)</b>
           is trained on feature representations from the text encoder of the diffusion model. 
          Once trained, it serves as a Concept Steerer, enabling precise, surgical concept manipulation by adjusting &lambda;.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Steering towards safety</h2>
        <div class="content has-text-justified">
          <img src="static/images/i2p.png" width="80%" style="display: block; margin: auto;">
        </div>
        <p class="content has-text-justified">
          <b>Qualitative comparisons of different approaches</b>
          , including TraSCE and SAFREE, on the I2P dataset. Our method removes nudity without significantly altering the generated images, resulting in outputs that are better aligned with the input prompt.
        </p>

        <div class="content has-text-justified">
          <img src="static/images/i2p_2.png" width="50%" style="display: block; margin: auto;">
        </div>
        <p class="content has-text-justified">
          <b>Qualitative examples from the I2P dataset.</b>
          Our method allows fine-grained control over the removal of specific concepts, removing only the intended concept while preserving the overall structure and style of the generated images.
        </p>

        <div class="content has-text-justified">
          <img src="static/images/p4d.png" width="80%" style="display: block; margin: auto;">
        </div>
        <p class="content has-text-justified">
          <b>Qualitative comparisons of different methods</b>
          , including TraSCE and SAFREE, on the P4D dataset. The P4D dataset consists of adversarial prompts designed to challenge generative models. Our approach effectively removes the concept of nudity during the generation process, producing safe and semantically meaningful outputs. In contrast, SAFREE fails to generate safe images, while TraSCE sometimes produces unrelated outputs despite the presence of semantically meaningful keywords in given prompts, such as "girl," "roman," "renaissance," and "paintings" (middle row).
        </p>

        <div class="content has-text-justified">
          <img src="static/images/flux.png" width="80%" style="display: block; margin: auto;">
        </div>
        <div class="content has-text-justified">
          <img src="static/images/app_flux.png" width="80%" style="display: block; margin: auto;">
        </div>
        <p class="content has-text-justified">
          <b>Qualitative example from the I2P dataset with FLUX</b>
          . Our method is model-agnostic and can be applied to both U-Net-based SD 1.4 and SDXL-Turbo, as well as DiT-based FLUX.
        </p>

        <div class="content has-text-justified">
          <img src="static/images/violence.png" width="30%" style="display: block; margin: auto;">
        </div>
        <div class="content has-text-justified">
          <img src="static/images/app_viol.png" width="80%" style="display: block; margin: auto;">
        </div>
        <p class="content has-text-justified">
          <b>Qualitative examples from the Ring-A-Bell dataset</b>
          . Our method successfully removes the abstract concept of violence, as shown by the absence of blood in the right images. The images are intentionally blurred for display purposes as they are disturbing.
        </p>

      </div>
    </div>
  </div>
</section>



<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Steering of photographic styles and object attributes</h2>
        <div class="content has-text-justified">
          <img src="static/images/minimalist_zoomin.png" width="80%" style="display: block; margin: auto;">
        </div>
        <p class="content has-text-justified">
          <b>Photographic style manipulation of SD 1.4 </b>
          for the given prompt "geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k, " 
          where concept prompts are "minimalist" (Top) and  "zoom-in, magnify" (Bottom), respectively. 
          In the top row, the image is manipulated toward a maximalist style as &lambda; &rarr; -1, 
          while it adopts a minimalist style as &lambda; &rarr; 1. Similarly, in the bottom row, 
          the image appears zoomed out and becomes blurred as &lambda; &rarr; -1, whereas it becomes zoomed in and clearer as &lambda; &rarr; 1.
        </p>

        <div class="content has-text-justified">
          <img src="static/images/sliders.png" width="80%" style="display: block; margin: auto;">
        </div>
        <p class="content has-text-justified">
          <b>Qualitative comparisons with weather Concept Sliders on SDXL-Turbo. </b>
          Note that Concept Sliders train specific sliders: winter weather slider and a dark weather slider, whereas our method trains a k-SAE 
          <b>only once</b> for different concepts. <b>Top:</b> "A photo of a tree with a bench, realistic, 8k" with concept to steer = "winter." <b>Bottom:</b> "A photo of a forest, realistic, 8k" with the concept to steer = "low light." Notice how in the top image our method also removes leaves while in the bottom image, our method effectively applies a low-light effect to the original image.
        </p>        

      <div class="content has-text-justified">
        <img src="static/images/dog.png" width="80%" style="display: block; margin: auto;">
      </div>
      <p class="content has-text-justified">
        <b>Image composition manipulation using SDXL-Turbo </b>
        for the prompt "A dog" with the concept prompt "Full shot." Notice how as &lambda; &rarr; 1, the generated image transitions from a close-up of the face to a full shot.
      </p>        

      <div class="content has-text-justified">
        <img src="static/images/car.png" width="80%" style="display: block; margin: auto;">
      </div>
      <div class="content has-text-justified">
        <img src="static/images/trees.png" width="80%" style="display: block; margin: auto;">
      </div>
      <p class="content has-text-justified">
        <b>Object attribute manipulation of SDXL-Turbo </b>
        for the given prompts "A car" (Top) and "A photo of a tree" (Bottom), where the concept prompts are "A blue car" (Top) and "Tree with autumn leaves" (Bottom). 
        By adjusting &lambda;, our method transitions the image toward the desired concept specified by the prompts.
      </p> 

      <div class="content has-text-justified">
        <img src="static/images/cakes.png" width="100%" style="display: block; margin: auto;">
      </div>
      <p class="content has-text-justified">
        <b>Object attribute manipulation of SDXL-Turbo </b>
        for the given prompts "A photo of a cake, 4k,"" where the concept prompts are "A chocolate cake," "A white cake," "A lemon cake," and "An orange cake," respectively. 
        By adjusting &lambda;, our method transitions the image toward the desired concept specified by the prompts.
      </p> 


    </div>
  </div>
</section>






<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
